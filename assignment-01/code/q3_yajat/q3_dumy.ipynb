{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all import statements\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model_directory = 'saved_models'\n",
    "# model_filename = 'linear_regression_model.pth'\n",
    "# model_file_path = os.path.join(model_directory, model_filename)\n",
    "\n",
    "# # Ensure the directory exists, create it if it doesn't\n",
    "# os.makedirs(model_directory, exist_ok=True)\n",
    "# # Function to load or initialize the model\n",
    "# def load_or_initialize_model(X_train, y_train):\n",
    "\n",
    "#     # Load the model if it exists, otherwise initialize a new model\n",
    "#     if os.path.exists(model_file_path):\n",
    "#         print('Model exist so using it')\n",
    "#         # input_size = X_train.shape[1]\n",
    "#         # model = nn.Linear(input_size, 1).double()\n",
    "#         # torch.save(model, model_file_path)\n",
    "#         model = torch.load(model_file_path)\n",
    "#     else:\n",
    "#         print('Model does not exist so initialising from scratch')\n",
    "#         input_size = X_train.shape[1]\n",
    "#         model = nn.Linear(input_size, 1).double()\n",
    "#         torch.save(model, model_file_path)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to train the model\n",
    "def modeltrain(X_train, y_train,l2_lambda=0.01):  # Specify the regularization strength (lambda)\n",
    "    # global model\n",
    "    input_size = X_train.shape[1]\n",
    "    model = nn.Linear(input_size, 1).double()  # Double data type for weight tensor\n",
    "\n",
    "    # Define loss function and optimizer with L2 regularization\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=l2_lambda)  # Adding weight decay for L2 regularization\n",
    "\n",
    "    # Train the model (rest of the code remains the same)\n",
    "    num_epochs = 1000\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass\n",
    "        outputs = model(X_train.double())  # Ensure input data type matches model's weight data type\n",
    "        loss = criterion(outputs, y_train.view(-1, 1))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "    print('Training Done')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to test the model\n",
    "\n",
    "def modeltest(X_test,y_test,model):\n",
    "    print(\"Testing starting\")\n",
    "    \n",
    "    # Assuming 'model' is your trained linear regression model\n",
    " \n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test)\n",
    "\n",
    "\n",
    "\n",
    "    print('testing over now accuracy analysis')\n",
    "\n",
    "    correct_pred=0\n",
    "    for i in range(len(predictions)):\n",
    "       \n",
    "        if(abs(predictions[i]-y_test[i])<0.5):\n",
    "            \n",
    "            correct_pred+=1\n",
    "    mean_accuracy=(correct_pred/len(predictions))*100\n",
    "    print(f'Percentage Accuracy: {mean_accuracy:.2f}%')\n",
    "\n",
    "    #loop used to extract few records for prediction of model on new data points\n",
    "    # for i in range(8):\n",
    "    #     print(\"prediction=\",predictions[i],\"target=\",y_test[i],X_test[i][0])\n",
    "    \n",
    "\n",
    "    #Calculation of mae,mse and r2 squared \n",
    "    mae = (mean_absolute_error(y_test, predictions))\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions.numpy())\n",
    "    \n",
    "\n",
    "    print(f'Mean Absolute Error: {mae}')\n",
    "    print(f'Mean Squared Error: {mse}')\n",
    "    print(f'R-squared: {r2}')\n",
    "\n",
    "    # Printing  the model parameters (coefficients)\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f'{name}: {param.data.numpy()}')\n",
    "\n",
    "    return predictions\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_plot(X,y,z):\n",
    "   \n",
    "    feature_to_plot=[]\n",
    "    list2=[]\n",
    "    for i in range(len(X)):\n",
    "        feature_to_plot.append(i+1)\n",
    "        \n",
    "    feature_to_plot1 = X[:,0] \n",
    "    feature_to_plot2 = X[:,1] \n",
    "\n",
    "    # feature_to_plot3 = X[:,2] \n",
    "    for i in range(len(y)):\n",
    "        # list2.append([y[i].item(),z[i].item()])\n",
    "         list2.append([z[i].item(),y[i].item()])\n",
    "    # print(list2)\n",
    "    list2.sort()\n",
    "    y=[]\n",
    "    z=[]\n",
    "    for i in range(len(list2)):\n",
    "        y.append(list2[i][1])\n",
    "        z.append(list2[i][0])\n",
    "\n",
    "\n",
    "    # Create a scatter plotc.ear\n",
    "   \n",
    "    plt.scatter(feature_to_plot,y,label='label(y)',alpha=0.5,color='red')\n",
    "\n",
    "    # # plt.scatter(feature_to_plot3, y, alpha=0.5)  #line plot\n",
    "    #Creating plot with label and axis name\n",
    "    plt.plot(feature_to_plot,z,label='prediction(z)',alpha=0.5)\n",
    "    # plt.plot(y,z,alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.xlabel('Data Point')  # Replace with the appropriate feature name\n",
    "    plt.ylabel('prediction(z),label(y)')\n",
    "    plt.title('Prediction(z),label(y) vs. Data Point')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Data  proccessing and normalisation tried\n",
    "\n",
    "df = pd.read_csv('Q3_complex_linear_regression_dataset.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define a mapping from categories to numerical values\n",
    "category_mapping = {'A': 1.0, 'B': 2.0, 'C': 3.0}\n",
    "df['X3'] = df['X3'].map(category_mapping)\n",
    "\n",
    "# Separate the target variable 'y' from the input features.\n",
    "X = df.drop(columns=['y'])\n",
    "y = df['y']\n",
    "\n",
    "for column in X.columns:\n",
    "    try:\n",
    "        X[column] = X[column].astype(float)\n",
    "    except ValueError:\n",
    "        print(f\"Column '{column}' cannot be converted to float.\")\n",
    "\n",
    "\n",
    "\n",
    "# Convert the DataFrame to a PyTorch tensor.\n",
    "X = torch.tensor(X.values, dtype=torch.float64)\n",
    "# Convert the target variable to a PyTorch tensor.\n",
    "y = torch.tensor(y.values, dtype=torch.float64)\n",
    "X1=X\n",
    "\n",
    "\n",
    "\n",
    "size=len(X)\n",
    "train_ratio=0.7\n",
    "train_size=int(size*train_ratio)\n",
    "test_size=size-train_size\n",
    "\n",
    "\n",
    "#Splitting the data in training and testing data \n",
    "\n",
    "\n",
    "#Training data\n",
    "X_train=X[:train_size]\n",
    "y_train=y[:train_size]\n",
    "\n",
    "#Testing data\n",
    "X_test=X[train_size:]\n",
    "y_test=y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance between X1 and X2: 0.12740488844015435\n",
      "Covariance between X2 and X3: 0.015396627005088991\n",
      "Covariance between X3 and X1: -0.014171793119161571\n",
      "Covariance between X1 and y: 24.942920604841778\n",
      "Covariance between X2 and y: 0.9254363356859601\n",
      "Covariance between X3 and y: 0.11293652151999131\n",
      "---------------------------------------------------\n",
      "Correlation Coefficient between X1 and y: 0.97\n",
      "Correlation Coefficient between X2 and y: 0.20\n",
      "Correlation Coefficient between X3 and y: 0.01\n"
     ]
    }
   ],
   "source": [
    "#Relation of input variables with each other (EDA)\n",
    "input1=[]\n",
    "input2=[]\n",
    "input3=[]\n",
    "output=[]\n",
    "for i in range(len(X_train)):\n",
    "    c=X_train[i][0]\n",
    "\n",
    "    input1.append(c.item())\n",
    "    c=X_train[i][1]\n",
    "\n",
    "    input2.append(c.item())\n",
    "    c=X_train[i][2]\n",
    "    input3.append(c.item())\n",
    "    c=y_train[i]\n",
    "    output.append(c.item())\n",
    "\n",
    "\n",
    "covariance_matrix = np.cov(input1, input2)\n",
    "\n",
    "# Extract the covariance between the two variables from the covariance matrix\n",
    "\n",
    "\n",
    "# print(\"Covariance between input1 and input2:\", covariance)\n",
    "\n",
    "covariance_matrix1 = np.cov(input1, input3)\n",
    "covariance_matrix2 = np.cov(input3, input2)\n",
    "covariance_matrix3=np.cov(input1, output)\n",
    "covariance_matrix4=np.cov(input2, output)\n",
    "covariance_matrix5=np.cov(input3, output)\n",
    "covariance = covariance_matrix[0, 1]\n",
    "covariance1 = covariance_matrix1[0, 1]\n",
    "covariance2 = covariance_matrix2[0, 1]\n",
    "covariance3 = covariance_matrix3[0, 1]\n",
    "covariance4 = covariance_matrix4[0, 1]\n",
    "covariance5 = covariance_matrix5[0, 1]\n",
    "\n",
    "#Extracting correlation factor between target and each input\n",
    "correlation_coefficient1 = np.corrcoef(input1, y_train)[0, 1]\n",
    "correlation_coefficient2 = np.corrcoef(input2, y_train)[0, 1]\n",
    "correlation_coefficient3 = np.corrcoef(input3, y_train)[0, 1]\n",
    "\n",
    "\n",
    "\n",
    "print(\"Covariance between X1 and X2:\", covariance)\n",
    "print(\"Covariance between X2 and X3:\", covariance1)\n",
    "print(\"Covariance between X3 and X1:\", covariance2)\n",
    "print(\"Covariance between X1 and y:\", covariance3)\n",
    "print(\"Covariance between X2 and y:\", covariance4)\n",
    "print(\"Covariance between X3 and y:\", covariance5)\n",
    "\n",
    "print(\"---------------------------------------------------\")\n",
    "\n",
    "print(f\"Correlation Coefficient between X1 and y: {correlation_coefficient1:.2f}\")\n",
    "print(f\"Correlation Coefficient between X2 and y: {correlation_coefficient2:.2f}\")\n",
    "print(f\"Correlation Coefficient between X3 and y: {correlation_coefficient3:.2f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_plot(X_train,y_train)\n",
    "# model = load_or_initialize_model(X_train,y_train)\n",
    "model=None\n",
    "def all_features():\n",
    "    global model\n",
    "    global X_test\n",
    "    model=modeltrain(X_train, y_train,l2_lambda=0.01)  # You can adjust the regularization st\n",
    "\n",
    "    pred=modeltest(X_test,y_test,model)\n",
    "\n",
    "    create_plot(X_test,y_test,pred)\n",
    "    # return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function  to remove any one features and train the model\n",
    "\n",
    "def remove_one_feature(feature1):\n",
    "    #Data  proccessing and normalisation tried\n",
    "\n",
    "    df = pd.read_csv('Q3_complex_linear_regression_dataset.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Define a mapping from categories to numerical values\n",
    "    if(feature1!='X3'):\n",
    "    \n",
    "        category_mapping = {'A': 1.0, 'B': 2.0, 'C': 3.0}\n",
    "        df['X3'] = df['X3'].map(category_mapping)\n",
    "\n",
    "    # Separate the target variable 'y' from the input features.\n",
    "    X = df.drop(columns=['y',feature1])\n",
    "    y = df['y']\n",
    "\n",
    "    for column in X.columns:\n",
    "        try:\n",
    "            X[column] = X[column].astype(float)\n",
    "        except ValueError:\n",
    "            print(f\"Column '{column}' cannot be converted to float.\")\n",
    "\n",
    "\n",
    "\n",
    "    # Convert the DataFrame to a PyTorch tensor.\n",
    "    X = torch.tensor(X.values, dtype=torch.float64)\n",
    "    # Convert the target variable to a PyTorch tensor.\n",
    "    y = torch.tensor(y.values, dtype=torch.float64)\n",
    "    X1=X\n",
    "\n",
    "\n",
    "    size=len(X)\n",
    "    train_ratio=0.7\n",
    "    train_size=int(size*train_ratio)\n",
    "    # test_size=size-train_size\n",
    "\n",
    "\n",
    "    #Splitting the data in training and testing data \n",
    "\n",
    "\n",
    "    #Training data\n",
    "    X_train=X[:train_size]\n",
    "    y_train=y[:train_size]\n",
    "\n",
    "    #Testing data\n",
    "    global X_test\n",
    "    X_test=X[train_size:]\n",
    "    y_test=y[train_size:]\n",
    "\n",
    "    global model\n",
    "    \n",
    "    model=modeltrain(X_train, y_train,l2_lambda=0.01)  # You can adjust the regularization st\n",
    "\n",
    "    pred=modeltest(X_test,y_test,model)\n",
    "\n",
    "    create_plot(X_test,y_test,pred)\n",
    "    \n",
    "    # return X_test\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function  to remove any two features and train the model\n",
    "\n",
    "def remove_two_feature(feature1,feature2):\n",
    "\n",
    "    #Data  proccessing and normalisation tried\n",
    "    df = pd.read_csv('Q3_complex_linear_regression_dataset.csv')\n",
    "    global X_test\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    # Define a mapping from categories to numerical values\n",
    "    if(feature1!='X3' and feature2!='X3'):\n",
    "        category_mapping = {'A': 1.0, 'B': 2.0, 'C': 3.0}\n",
    "        df['X3'] = df['X3'].map(category_mapping)\n",
    "\n",
    "    # Separate the target variable 'y' from the input features.\n",
    "    X = df.drop(columns=['y',feature1,feature2])\n",
    "    y = df['y']\n",
    "    y = df['y']\n",
    "\n",
    "    for column in X.columns:\n",
    "        try:\n",
    "            X[column] = X[column].astype(float)\n",
    "        except ValueError:\n",
    "            print(f\"Column '{column}' cannot be converted to float.\")\n",
    "\n",
    "\n",
    "\n",
    "    # Convert the DataFrame to a PyTorch tensor.\n",
    "    X = torch.tensor(X.values, dtype=torch.float64)\n",
    "    # Convert the target variable to a PyTorch tensor.\n",
    "    y = torch.tensor(y.values, dtype=torch.float64)\n",
    "    X1=X\n",
    "   \n",
    "\n",
    "    size=len(X)\n",
    "    train_ratio=0.7\n",
    "    train_size=int(size*train_ratio)\n",
    "    # test_size=size-train_size\n",
    "\n",
    "\n",
    "    #Splitting the data in training and testing data \n",
    "\n",
    "\n",
    "    #Training data\n",
    "    X_train=X[:train_size]\n",
    "    y_train=y[:train_size]\n",
    "\n",
    "    #Testing data\n",
    "    X_test=X[train_size:]\n",
    "    y_test=y[train_size:]\n",
    "\n",
    "    global model\n",
    "    model=modeltrain(X_train, y_train,l2_lambda=0.01)  # You can adjust the regularization st\n",
    "\n",
    "    pred=modeltest(X_test,y_test,model)\n",
    "    # create_plot(X_test,y_test,pred)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 5.053461152506122\n",
      "Epoch [200/1000], Loss: 5.040747072530288\n",
      "Epoch [300/1000], Loss: 5.036038451282846\n",
      "Epoch [400/1000], Loss: 5.03430451914318\n",
      "Epoch [500/1000], Loss: 5.033672103675417\n",
      "Epoch [600/1000], Loss: 5.03344522481673\n",
      "Epoch [700/1000], Loss: 5.033366199421993\n",
      "Epoch [800/1000], Loss: 5.033340180397723\n",
      "Epoch [900/1000], Loss: 5.033332601384615\n",
      "Epoch [1000/1000], Loss: 5.0333310786982315\n",
      "Training Done\n",
      "Testing starting\n",
      "testing over now accuracy analysis\n",
      "Percentage Accuracy: 18.89%\n",
      "Mean Absolute Error: 1.7371926510750573\n",
      "Mean Squared Error: 4.648724295753463\n",
      "R-squared: 0.9491825457177125\n",
      "weight: [[3.0810531]]\n",
      "bias: [0.62833837]\n"
     ]
    }
   ],
   "source": [
    "### You can uncomment these two to run the code for model predictions by removing one and two features respectively.\n",
    "# remove_one_feature('X3')\n",
    "remove_two_feature('X2','X3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.9034], dtype=torch.float64)\n",
      "Testing starting\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "both arguments to matmul need to be at least 1D, but they are 0D and 2D",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\pande\\OneDrive\\Desktop\\code\\ML\\ELL409-Machine-Learning-Coursework\\assignment-01\\code\\q3_yajat\\q3_dumy.ipynb Cell 13\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pande/OneDrive/Desktop/code/ML/ELL409-Machine-Learning-Coursework/assignment-01/code/q3_yajat/q3_dumy.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         pred\u001b[39m=\u001b[39mmodeltest(X_test[testing_row][\u001b[39m0\u001b[39m],y_test[testing_row],model)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pande/OneDrive/Desktop/code/ML/ELL409-Machine-Learning-Coursework/assignment-01/code/q3_yajat/q3_dumy.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# test_single()\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/pande/OneDrive/Desktop/code/ML/ELL409-Machine-Learning-Coursework/assignment-01/code/q3_yajat/q3_dumy.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m test_single(\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pande/OneDrive/Desktop/code/ML/ELL409-Machine-Learning-Coursework/assignment-01/code/q3_yajat/q3_dumy.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m X_test\u001b[39m.\u001b[39mshape\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pande/OneDrive/Desktop/code/ML/ELL409-Machine-Learning-Coursework/assignment-01/code/q3_yajat/q3_dumy.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# y_test\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pande/OneDrive/Desktop/code/ML/ELL409-Machine-Learning-Coursework/assignment-01/code/q3_yajat/q3_dumy.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# model\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\pande\\OneDrive\\Desktop\\code\\ML\\ELL409-Machine-Learning-Coursework\\assignment-01\\code\\q3_yajat\\q3_dumy.ipynb Cell 13\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pande/OneDrive/Desktop/code/ML/ELL409-Machine-Learning-Coursework/assignment-01/code/q3_yajat/q3_dumy.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pande/OneDrive/Desktop/code/ML/ELL409-Machine-Learning-Coursework/assignment-01/code/q3_yajat/q3_dumy.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(X_test[testing_row])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/pande/OneDrive/Desktop/code/ML/ELL409-Machine-Learning-Coursework/assignment-01/code/q3_yajat/q3_dumy.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     pred\u001b[39m=\u001b[39mmodeltest(X_test[testing_row][\u001b[39m0\u001b[39;49m],y_test[testing_row],model)\n",
      "\u001b[1;32mc:\\Users\\pande\\OneDrive\\Desktop\\code\\ML\\ELL409-Machine-Learning-Coursework\\assignment-01\\code\\q3_yajat\\q3_dumy.ipynb Cell 13\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pande/OneDrive/Desktop/code/ML/ELL409-Machine-Learning-Coursework/assignment-01/code/q3_yajat/q3_dumy.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Assuming 'model' is your trained linear regression model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pande/OneDrive/Desktop/code/ML/ELL409-Machine-Learning-Coursework/assignment-01/code/q3_yajat/q3_dumy.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/pande/OneDrive/Desktop/code/ML/ELL409-Machine-Learning-Coursework/assignment-01/code/q3_yajat/q3_dumy.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     predictions \u001b[39m=\u001b[39m model(X_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pande/OneDrive/Desktop/code/ML/ELL409-Machine-Learning-Coursework/assignment-01/code/q3_yajat/q3_dumy.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtesting over now accuracy analysis\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pande/OneDrive/Desktop/code/ML/ELL409-Machine-Learning-Coursework/assignment-01/code/q3_yajat/q3_dumy.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m correct_pred\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\pande\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\pande\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: both arguments to matmul need to be at least 1D, but they are 0D and 2D"
     ]
    }
   ],
   "source": [
    "\n",
    "def test_single(testing_row=None):\n",
    "    if testing_row is None:\n",
    "        # print(y_test.shape)\n",
    "        pred=modeltest(X_test,y_test,model)\n",
    "    else:\n",
    "        print(X_test[testing_row])\n",
    "        pred=modeltest(X_test[testing_row][0],y_test[testing_row],model)\n",
    "# test_single()\n",
    "test_single(0)\n",
    "X_test.shape\n",
    "\n",
    "# y_test\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.16896984398474"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_test[1]\n",
    "# X_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
