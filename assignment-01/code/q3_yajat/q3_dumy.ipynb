{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all import statements\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_directory = 'saved_models'\n",
    "model_filename = 'linear_regression_model.pth'\n",
    "model_file_path = os.path.join(model_directory, model_filename)\n",
    "\n",
    "# Ensure the directory exists, create it if it doesn't\n",
    "os.makedirs(model_directory, exist_ok=True)\n",
    "# Function to load or initialize the model\n",
    "def load_or_initialize_model(X_train, y_train):\n",
    "\n",
    "    # Load the model if it exists, otherwise initialize a new model\n",
    "    if os.path.exists(model_file_path):\n",
    "        print('Model exist so using it')\n",
    "        input_size = X_train.shape[1]\n",
    "        model = nn.Linear(input_size, 1).double()\n",
    "        torch.save(model, model_file_path)\n",
    "        # model = torch.load(model_file_path)\n",
    "    else:\n",
    "        print('Model does not exist so initialising from scratch')\n",
    "        input_size = X_train.shape[1]\n",
    "        model = nn.Linear(input_size, 1).double()\n",
    "        torch.save(model, model_file_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to train the model\n",
    "def modeltrain(X_train, y_train, l2_lambda=0.01):  # Specify the regularization strength (lambda)\n",
    "\n",
    "    input_size = X_train.shape[1]\n",
    "    model = nn.Linear(input_size, 1).double()  # Double data type for weight tensor\n",
    "\n",
    "    # Define loss function and optimizer with L2 regularization\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=l2_lambda)  # Adding weight decay for L2 regularization\n",
    "\n",
    "    # Train the model (rest of the code remains the same)\n",
    "    num_epochs = 1000\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass\n",
    "        outputs = model(X_train.double())  # Ensure input data type matches model's weight data type\n",
    "        loss = criterion(outputs, y_train.view(-1, 1))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "    print('Training Done')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to train the model with L3 regularization\n",
    "# def modeltrain(X_train, y_train, l2_lambda=0.01):\n",
    "#     input_size = X_train.shape[1]\n",
    "#     model = nn.Linear(input_size, 1).double()\n",
    "\n",
    "#     # Define loss function and optimizer with L3 regularization\n",
    "#     criterion = nn.MSELoss()\n",
    "#     optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=l2_lambda)  # No weight decay for L2 regularization\n",
    "\n",
    "#     # Training parameters\n",
    "#     num_epochs = 1000\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Forward pass\n",
    "#         outputs = model(X_train.double())  # Ensure input data type matches model's weight data type\n",
    "#         loss = criterion(outputs, y_train.view(-1, 1))\n",
    "\n",
    "#         # Add L3 regularization term to the loss function\n",
    "#         # l3_regularization = 0\n",
    "#         # for param in model.parameters():\n",
    "#         #     l3_regularization += torch.norm(param, 3)  # L3 norm of the parameters\n",
    "\n",
    "#         # # Combine loss with L3 regularization\n",
    "#         # loss += l3_lambda * l3_regularization\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         if (epoch + 1) % 100 == 0:\n",
    "#             print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "#     print('Training Done')\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to test the model\n",
    "\n",
    "def modeltest(X_test,y_test,model):\n",
    "    print(\"Testing starting\")\n",
    "    # Assuming 'model' is your trained linear regression model\n",
    "    # print(model)\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test)\n",
    "\n",
    "\n",
    "\n",
    "    print('testing over now accuracy analysis')\n",
    "    # print(predictions)\n",
    "    correct_pred=0\n",
    "    for i in range(len(predictions)):\n",
    "        if(abs(predictions[i]-y_test[i])<0.5):\n",
    "            correct_pred+=1\n",
    "    mean_accuracy=(correct_pred/len(predictions))*100\n",
    "    print(f'Percentage Accuracy: {mean_accuracy:.2f}%')\n",
    "    \n",
    "\n",
    "    #Calculation of mae,mse and r2 squared \n",
    "    mae = (mean_absolute_error(y_test, predictions))\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions.numpy())\n",
    "    \n",
    "\n",
    "    print(f'Mean Absolute Error: {mae}')\n",
    "    print(f'Mean Squared Error: {mse}')\n",
    "    print(f'R-squared: {r2}')\n",
    "\n",
    "    # Printing  the model parameters (coefficients)\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f'{name}: {param.data.numpy()}')\n",
    "\n",
    "    return predictions\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_plot(X,y,z):\n",
    "    # Replace 'X1' with the name of the feature you want to plot.\n",
    "    # n=int(input(\"Enter the feature for which you want the plot, enter any value b/w 0-2:\"))\n",
    "    # n1=int(input(\"Enter the feature for which you want the plot, enter any value b/w 0-2:\"))\n",
    "    # n2=int(input(\"Enter the feature for which you want the plot, enter any value b/w 0-2:\"))\n",
    "\n",
    "    feature_to_plot1 = X[:,0] \n",
    "    feature_to_plot2 = X[:,1] \n",
    "    # feature_to_plot3 = X[:,2] \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # Create a scatter plotc.ear\n",
    "    # plt.scatter(feature_to_plot, y, alpha=0.5)\n",
    "    # plt.show()\n",
    "    # plt.scatter(feature_to_plot2, y, alpha=0.5)\n",
    "    # plt.show()\n",
    "    plt.scatter(feature_to_plot1, y, alpha=0.5)\n",
    "\n",
    "    # plt.scatter(feature_to_plot3, y, alpha=0.5)  #line plot\n",
    "    plt.plot(feature_to_plot1,z,alpha=0.5)\n",
    "    plt.xlabel('Feature No:')  # Replace with the appropriate feature name\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Line Plot of Feature vs. y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           X1  X2\n",
      "0    5.488135   0\n",
      "1    7.151894   0\n",
      "2    6.027634   1\n",
      "3    5.448832   1\n",
      "4    4.236548   1\n",
      "..        ...  ..\n",
      "295  2.243170   1\n",
      "296  0.978445   0\n",
      "297  8.621915   1\n",
      "298  9.729195   0\n",
      "299  9.608347   1\n",
      "\n",
      "[300 rows x 2 columns]\n",
      "tensor([34.2294, 10.2902, 20.9036,  6.5328, -1.1335, 22.7401, 12.5336, 12.0523,\n",
      "        14.1839, 26.3314, 18.9335, 26.6021,  8.3810, 27.2425,  6.3316, 30.7620,\n",
      "        23.1129,  6.7759, 26.6160, 23.5592, 11.0022,  5.8888, 20.0495,  1.0064,\n",
      "        11.2041, 15.7228, 13.4056, 15.8035, 10.5840, 17.6836, 23.4477,  4.0655,\n",
      "        12.5672,  6.1347, 26.0696, 10.9410, 14.9168,  9.8264,  3.2906, 20.2204,\n",
      "        13.9066, 28.0159, 26.1350, 27.3269, 25.0362,  2.4789, 16.4772, 20.5333,\n",
      "        30.8378,  9.8390,  6.8636,  2.3062, -0.0739, 31.2607, 21.9871, 26.4835,\n",
      "        12.8907, 18.4733,  0.3192, 16.4449, 30.2821, 27.3789, 11.5924, 30.8640,\n",
      "         9.7869, 29.0457, 33.5439, 25.4760, 21.5939, 25.3474,  4.8415, 22.4825,\n",
      "        19.6559,  2.2372, 10.3528,  5.9413, 34.5543, 15.1747, 18.1470, 18.8038,\n",
      "        15.5560,  6.8865, 25.6411,  6.9998, 15.7739,  5.7106,  3.5375, 28.6174,\n",
      "        27.8248, 32.2664], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Data  proccessing and normalisation tried\n",
    "\n",
    "df = pd.read_csv('Q3_complex_linear_regression_dataset.csv')\n",
    "#print(df.head())\n",
    "\n",
    "\n",
    "\n",
    "# # Separate the target variable 'y' from the input features.\n",
    "# X = df.drop(columns=['y'])\n",
    "# y = df['y']\n",
    "\n",
    "# # Handle categorical variables using one-hot encoding.\n",
    "# X = pd.get_dummies(X, columns=['X3'], prefix=['X3'])\n",
    "\n",
    "# Define a mapping from categories to numerical values\n",
    "category_mapping = {'A': 1.0, 'B': 2.0, 'C': 3.0}\n",
    "df['X3'] = df['X3'].map(category_mapping)\n",
    "\n",
    "# Separate the target variable 'y' from the input features.\n",
    "X = df.drop(columns=['y'])\n",
    "y = df['y']\n",
    "\n",
    "for column in X.columns:\n",
    "    try:\n",
    "        X[column] = X[column].astype(float)\n",
    "    except ValueError:\n",
    "        print(f\"Column '{column}' cannot be converted to float.\")\n",
    "\n",
    "\n",
    "\n",
    "# Convert the DataFrame to a PyTorch tensor.\n",
    "X = torch.tensor(X.values, dtype=torch.float64)\n",
    "# Convert the target variable to a PyTorch tensor.\n",
    "y = torch.tensor(y.values, dtype=torch.float64)\n",
    "X1=X\n",
    "# print('X torch vector is as follows')\n",
    "# print(X)\n",
    "# list_X1=[]\n",
    "# listd=[]\n",
    "# for i in range(len(X)):\n",
    "#     list_X1.append(X[i][0].item())\n",
    "#     listd.append([X[i][0].item(),X[i][1].item(),X[i][2].item()])\n",
    "\n",
    "\n",
    "# # print(list_X1)\n",
    "# list_X1=torch.tensor(list_X1)\n",
    "# mean_X1 = torch.mean(list_X1).item()\n",
    "# std_X1 = torch.std(list_X1).item()\n",
    "# # print(mean_X1.item(),std_X1.item())\n",
    "\n",
    "# for i in range(len(X)):\n",
    "#     listd[i][0]=(listd[i][0]-mean_X1)/std_X1\n",
    "# listd=torch.tensor(listd)\n",
    "# print(\"X befor normalisation\",X)\n",
    "# X=listd\n",
    "# print('X after normalsation',X)\n",
    "\n",
    "\n",
    "size=len(X)\n",
    "train_ratio=0.7\n",
    "train_size=int(size*train_ratio)\n",
    "test_size=size-train_size\n",
    "\n",
    "\n",
    "#Splitting the data in training and testing data \n",
    "\n",
    "\n",
    "#Training data\n",
    "X_train=X[:train_size]\n",
    "y_train=y[:train_size]\n",
    "\n",
    "#Testing data\n",
    "X_test=X[train_size:]\n",
    "y_test=y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size=len(X)\n",
    "# train_ratio=1\n",
    "# train_size=int(size*train_ratio)\n",
    "# # test_size=size-train_size\n",
    "# test_size=train_size\n",
    "\n",
    "\n",
    "# #Splitting the data in training and testing data \n",
    "\n",
    "\n",
    "# #Training data\n",
    "# X_train=X[:train_size]\n",
    "# y_train=y[:train_size]\n",
    "\n",
    "# #Testing data\n",
    "# # X_test=X[train_size:]\n",
    "# # y_test=y[train_size:]\n",
    "# X_test=X_train\n",
    "# y_test=y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance between X1 and X2: 0.12740488844015435\n",
      "Covariance between X1 and y: 24.942920604841778\n",
      "Covariance between X2 and y: 0.9254363356859601\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "Correlation Coefficient for X1 and y: 0.97\n",
      "Correlation Coefficient for X2 and y: 0.20\n"
     ]
    }
   ],
   "source": [
    "#Relation of input variables with each other (EDA)\n",
    "input1=[]\n",
    "input2=[]\n",
    "input3=[]\n",
    "output=[]\n",
    "for i in range(len(X_train)):\n",
    "    c=X_train[i][0]\n",
    "\n",
    "    input1.append(c.item())\n",
    "    c=X_train[i][1]\n",
    "\n",
    "    input2.append(c.item())\n",
    "    c=X_train[i][2]\n",
    "    input3.append(c.item())\n",
    "    c=y_train[i]\n",
    "    output.append(c.item())\n",
    "\n",
    "\n",
    "covariance_matrix = np.cov(input1, input2)\n",
    "\n",
    "# Extract the covariance between the two variables from the covariance matrix\n",
    "\n",
    "\n",
    "# print(\"Covariance between input1 and input2:\", covariance)\n",
    "\n",
    "covariance_matrix1 = np.cov(input1, input3)\n",
    "covariance_matrix2 = np.cov(input3, input2)\n",
    "covariance_matrix3=np.cov(input1, output)\n",
    "covariance_matrix4=np.cov(input2, output)\n",
    "covariance_matrix5=np.cov(input3, output)\n",
    "covariance = covariance_matrix[0, 1]\n",
    "covariance1 = covariance_matrix1[0, 1]\n",
    "covariance2 = covariance_matrix2[0, 1]\n",
    "covariance3 = covariance_matrix3[0, 1]\n",
    "covariance4 = covariance_matrix4[0, 1]\n",
    "covariance5 = covariance_matrix5[0, 1]\n",
    "\n",
    "correlation_coefficient1 = np.corrcoef(input1, y_train)[0, 1]\n",
    "correlation_coefficient2 = np.corrcoef(input2, y_train)[0, 1]\n",
    "correlation_coefficient3 = np.corrcoef(input3, y_train)[0, 1]\n",
    "\n",
    "\n",
    "\n",
    "print(\"Covariance between X1 and X2:\", covariance)\n",
    "print(\"Covariance between X2 and X3:\", covariance1)\n",
    "print(\"Covariance between X3 and X1:\", covariance2)\n",
    "print(\"Covariance between X1 and y:\", covariance3)\n",
    "print(\"Covariance between X2 and y:\", covariance4)\n",
    "print(\"Covariance between X3 and y:\", covariance5)\n",
    "\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"---------------------------------------------------\")\n",
    "\n",
    "print(f\"Correlation Coefficient between X1 and y: {correlation_coefficient1:.2f}\")\n",
    "print(f\"Correlation Coefficient between X2 and y: {correlation_coefficient2:.2f}\")\n",
    "print(f\"Correlation Coefficient between X3 and y: {correlation_coefficient3:.2f}\")\n",
    "# print(f\"Correlation Coefficient: {correlation_coefficient:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 4.755609002070013e+161\n",
      "Epoch [200/1000], Loss: inf\n",
      "Epoch [300/1000], Loss: inf\n",
      "Epoch [400/1000], Loss: nan\n",
      "Epoch [500/1000], Loss: nan\n",
      "Epoch [600/1000], Loss: nan\n",
      "Epoch [700/1000], Loss: nan\n",
      "Epoch [800/1000], Loss: nan\n",
      "Epoch [900/1000], Loss: nan\n",
      "Epoch [1000/1000], Loss: nan\n",
      "Training Done\n",
      "Testing starting\n",
      "testing over now accuracy analysis\n",
      "Percentage Accuracy: 0.00%\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[244], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# create_plot(X_train,y_train)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# model = load_or_initialize_model(X_train,y_train)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m model \u001b[39m=\u001b[39m modeltrain(X_train, y_train, l2_lambda\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)  \u001b[39m# You can adjust the regularization st\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m pred\u001b[39m=\u001b[39mmodeltest(X_test,y_test,model)\n\u001b[1;32m      6\u001b[0m create_plot(X_test,y_test,pred)\n",
      "Cell \u001b[0;32mIn[239], line 23\u001b[0m, in \u001b[0;36mmodeltest\u001b[0;34m(X_test, y_test, model)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mPercentage Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mmean_accuracy\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[39m#Calculation of mae,mse and r2 squared \u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m mae \u001b[39m=\u001b[39m (mean_absolute_error(y_test, predictions))\n\u001b[1;32m     24\u001b[0m mse \u001b[39m=\u001b[39m mean_squared_error(y_test, predictions)\n\u001b[1;32m     25\u001b[0m r2 \u001b[39m=\u001b[39m r2_score(y_test, predictions\u001b[39m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_regression.py:204\u001b[0m, in \u001b[0;36mmean_absolute_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39m@validate_params\u001b[39m(\n\u001b[1;32m    141\u001b[0m     {\n\u001b[1;32m    142\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m\"\u001b[39m\u001b[39marray-like\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m     y_true, y_pred, \u001b[39m*\u001b[39m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, multioutput\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39muniform_average\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m ):\n\u001b[1;32m    152\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Mean absolute error regression loss.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \n\u001b[1;32m    154\u001b[0m \u001b[39m    Read more in the :ref:`User Guide <mean_absolute_error>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39m    0.85...\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[39m=\u001b[39m _check_reg_targets(\n\u001b[1;32m    205\u001b[0m         y_true, y_pred, multioutput\n\u001b[1;32m    206\u001b[0m     )\n\u001b[1;32m    207\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    208\u001b[0m     output_errors \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39maverage(np\u001b[39m.\u001b[39mabs(y_pred \u001b[39m-\u001b[39m y_true), weights\u001b[39m=\u001b[39msample_weight, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_regression.py:101\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m    100\u001b[0m y_true \u001b[39m=\u001b[39m check_array(y_true, ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m--> 101\u001b[0m y_pred \u001b[39m=\u001b[39m check_array(y_pred, ensure_2d\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    103\u001b[0m \u001b[39mif\u001b[39;00m y_true\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    104\u001b[0m     y_true \u001b[39m=\u001b[39m y_true\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    953\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    954\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    955\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    956\u001b[0m         )\n\u001b[1;32m    958\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 959\u001b[0m         _assert_all_finite(\n\u001b[1;32m    960\u001b[0m             array,\n\u001b[1;32m    961\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[1;32m    962\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[1;32m    963\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    964\u001b[0m         )\n\u001b[1;32m    966\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    967\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py:124\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    122\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m _assert_all_finite_element_wise(\n\u001b[1;32m    125\u001b[0m     X,\n\u001b[1;32m    126\u001b[0m     xp\u001b[39m=\u001b[39;49mxp,\n\u001b[1;32m    127\u001b[0m     allow_nan\u001b[39m=\u001b[39;49mallow_nan,\n\u001b[1;32m    128\u001b[0m     msg_dtype\u001b[39m=\u001b[39;49mmsg_dtype,\n\u001b[1;32m    129\u001b[0m     estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[1;32m    130\u001b[0m     input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[1;32m    131\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py:173\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    157\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    160\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m     )\n\u001b[0;32m--> 173\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "# create_plot(X_train,y_train)\n",
    "# model = load_or_initialize_model(X_train,y_train)\n",
    "model = modeltrain(X_train, y_train, l2_lambda=0.01)  # You can adjust the regularization st\n",
    "\n",
    "pred=modeltest(X_test,y_test,model)\n",
    "create_plot(X_test,y_test,pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
