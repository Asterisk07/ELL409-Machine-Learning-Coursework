{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all import statements\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_directory = 'saved_models'\n",
    "model_filename = 'linear_regression_model.pth'\n",
    "model_file_path = os.path.join(model_directory, model_filename)\n",
    "\n",
    "# Ensure the directory exists, create it if it doesn't\n",
    "os.makedirs(model_directory, exist_ok=True)\n",
    "# Function to load or initialize the model\n",
    "def load_or_initialize_model(X_train, y_train):\n",
    "\n",
    "    # Load the model if it exists, otherwise initialize a new model\n",
    "    if os.path.exists(model_file_path):\n",
    "        print('Model exist so using it')\n",
    "        input_size = X_train.shape[1]\n",
    "        model = nn.Linear(input_size, 1).double()\n",
    "        torch.save(model, model_file_path)\n",
    "        # model = torch.load(model_file_path)\n",
    "    else:\n",
    "        print('Model does not exist so initialising from scratch')\n",
    "        input_size = X_train.shape[1]\n",
    "        model = nn.Linear(input_size, 1).double()\n",
    "        torch.save(model, model_file_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #function to train the model\n",
    "# def modeltrain(X_train, y_train, l2_lambda=0.01):  # Specify the regularization strength (lambda)\n",
    "\n",
    "#     input_size = X_train.shape[1]\n",
    "#     model = nn.Linear(input_size, 1).double()  # Double data type for weight tensor\n",
    "\n",
    "#     # Define loss function and optimizer with L2 regularization\n",
    "#     criterion = nn.MSELoss()\n",
    "#     optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=l2_lambda)  # Adding weight decay for L2 regularization\n",
    "\n",
    "#     # Train the model (rest of the code remains the same)\n",
    "#     num_epochs = 1000\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Forward pass\n",
    "#         outputs = model(X_train.double())  # Ensure input data type matches model's weight data type\n",
    "#         loss = criterion(outputs, y_train.view(-1, 1))\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         if (epoch + 1) % 100 == 0:\n",
    "#             print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "#     print('Training Done')\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model with L3 regularization\n",
    "def modeltrain(X_train, y_train, l3_lambda=0.01):\n",
    "    input_size = X_train.shape[1]\n",
    "    model = nn.Linear(input_size, 1).double()\n",
    "\n",
    "    # Define loss function and optimizer with L3 regularization\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=0)  # No weight decay for L2 regularization\n",
    "\n",
    "    # Training parameters\n",
    "    num_epochs = 1000\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass\n",
    "        outputs = model(X_train.double())  # Ensure input data type matches model's weight data type\n",
    "        loss = criterion(outputs, y_train.view(-1, 1))\n",
    "\n",
    "        # Add L3 regularization term to the loss function\n",
    "        l3_regularization = 0\n",
    "        for param in model.parameters():\n",
    "            l3_regularization += torch.norm(param, 3)  # L3 norm of the parameters\n",
    "\n",
    "        # Combine loss with L3 regularization\n",
    "        loss += l3_lambda * l3_regularization\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "    print('Training Done')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to test the model\n",
    "def modeltest(X_test,y_test,model):\n",
    "    print(\"Testing starting\")\n",
    "    # Assuming 'model' is your trained linear regression model\n",
    "    # print(model)\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test)\n",
    "\n",
    "\n",
    "    print('testing over now accuracy analysis')\n",
    "    # print(predictions)\n",
    "    correct_pred=0\n",
    "    for i in range(len(predictions)):\n",
    "        if(abs(predictions[i]-y_test[i])<3):\n",
    "            correct_pred+=1\n",
    "    mean_accuracy=(correct_pred/len(predictions))*100\n",
    "    print(f'Percentage Accuracy: {mean_accuracy:.2f}%')\n",
    "\n",
    "    # Calculate MAE and MSE\n",
    "    # mae = mean_absolute_error(y_test, predictions)\n",
    "    # mse = mean_squared_error(y_test, predictions)\n",
    "\n",
    "    # print(f'MAE: {mae}')\n",
    "    # print(f'MSE: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_plot(X,y):\n",
    "    # Replace 'X1' with the name of the feature you want to plot.\n",
    "    # n=int(input(\"Enter the feature for which you want the plot, enter any value b/w 0-2:\"))\n",
    "    # n1=int(input(\"Enter the feature for which you want the plot, enter any value b/w 0-2:\"))\n",
    "    # n2=int(input(\"Enter the feature for which you want the plot, enter any value b/w 0-2:\"))\n",
    "\n",
    "    feature_to_plot = X[:,0] \n",
    "    feature_to_plot1 = X[:,1] \n",
    "    feature_to_plot2 = X[:,2] \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # Create a scatter plotc.ear\n",
    "    # plt.scatter(feature_to_plot, y, alpha=0.5)\n",
    "    # plt.show()\n",
    "    plt.scatter(feature_to_plot1, y, alpha=0.5)\n",
    "    # plt.show()\n",
    "    plt.scatter(feature_to_plot2, y, alpha=0.5)\n",
    "\n",
    "    # plt.plot(feature_to_plot, y, alpha=0.5)  #line plot\n",
    "    plt.xlabel('Feature No:')  # Replace with the appropriate feature name\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Line Plot of Feature vs. y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Data  proccessing and normalisation tried\n",
    "\n",
    "df = pd.read_csv('Q3_complex_linear_regression_dataset.csv')\n",
    "#print(df.head())\n",
    "\n",
    "\n",
    "\n",
    "# # Separate the target variable 'y' from the input features.\n",
    "# X = df.drop(columns=['y'])\n",
    "# y = df['y']\n",
    "\n",
    "# # Handle categorical variables using one-hot encoding.\n",
    "# X = pd.get_dummies(X, columns=['X3'], prefix=['X3'])\n",
    "\n",
    "# Define a mapping from categories to numerical values\n",
    "category_mapping = {'A': 1.0, 'B': 2.0, 'C': 3.0}\n",
    "df['X3'] = df['X3'].map(category_mapping)\n",
    "\n",
    "# Separate the target variable 'y' from the input features.\n",
    "X = df.drop(columns=['y'])\n",
    "y = df['y']\n",
    "\n",
    "for column in X.columns:\n",
    "    try:\n",
    "        X[column] = X[column].astype(float)\n",
    "    except ValueError:\n",
    "        print(f\"Column '{column}' cannot be converted to float.\")\n",
    "\n",
    "\n",
    "\n",
    "# Convert the DataFrame to a PyTorch tensor.\n",
    "X = torch.tensor(X.values, dtype=torch.float64)\n",
    "# Convert the target variable to a PyTorch tensor.\n",
    "y = torch.tensor(y.values, dtype=torch.float64)\n",
    "# print('X torch vector is as follows')\n",
    "# print(X)\n",
    "# list_X1=[]\n",
    "# listd=[]\n",
    "# for i in range(len(X)):\n",
    "#     list_X1.append(X[i][0].item())\n",
    "#     listd.append([X[i][0].item(),X[i][1].item(),X[i][2].item()])\n",
    "\n",
    "\n",
    "# # print(list_X1)\n",
    "# list_X1=torch.tensor(list_X1)\n",
    "# mean_X1 = torch.mean(list_X1).item()\n",
    "# std_X1 = torch.std(list_X1).item()\n",
    "# # print(mean_X1.item(),std_X1.item())\n",
    "\n",
    "# for i in range(len(X)):\n",
    "#     listd[i][0]=(listd[i][0]-mean_X1)/std_X1\n",
    "# listd=torch.tensor(listd)\n",
    "# print(\"X befor normalisation\",X)\n",
    "# X=listd\n",
    "# print('X after normalsation',X)\n",
    "\n",
    "\n",
    "# size=len(X)\n",
    "# train_ratio=0.7\n",
    "# train_size=int(size*train_ratio)\n",
    "# test_size=size-train_size\n",
    "\n",
    "\n",
    "# #Splitting the data in training and testing data \n",
    "\n",
    "\n",
    "# #Training data\n",
    "# X_train=X[:train_size]\n",
    "# y_train=y[:train_size]\n",
    "\n",
    "# #Testing data\n",
    "# X_test=X[train_size:]\n",
    "# y_test=y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=len(X)\n",
    "train_ratio=1\n",
    "train_size=int(size*train_ratio)\n",
    "# test_size=size-train_size\n",
    "test_size=train_size\n",
    "\n",
    "\n",
    "#Splitting the data in training and testing data \n",
    "\n",
    "\n",
    "#Training data\n",
    "X_train=X[:train_size]\n",
    "y_train=y[:train_size]\n",
    "\n",
    "#Testing data\n",
    "# X_test=X[train_size:]\n",
    "# y_test=y[train_size:]\n",
    "X_test=X_train\n",
    "y_test=y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance between input1 and input2: 0.10564498653977594\n",
      "Covariance between input2 and input3: -0.09403573031128368\n",
      "Covariance between input3 and input2: -0.03160535117056865\n"
     ]
    }
   ],
   "source": [
    "#Relation of input variables with each other (EDA)\n",
    "input1=[]\n",
    "input2=[]\n",
    "input3=[]\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    c=X_train[i][0]\n",
    "\n",
    "    input1.append(c.item())\n",
    "    c=X_train[i][1]\n",
    "\n",
    "    input2.append(c.item())\n",
    "    c=X_train[i][2]\n",
    "    input3.append(c.item())\n",
    "\n",
    "covariance_matrix = np.cov(input1, input2)\n",
    "\n",
    "# Extract the covariance between the two variables from the covariance matrix\n",
    "covariance = covariance_matrix[0, 1]\n",
    "\n",
    "# print(\"Covariance between input1 and input2:\", covariance)\n",
    "\n",
    "covariance_matrix1 = np.cov(input1, input3)\n",
    "covariance_matrix2 = np.cov(input3, input2)\n",
    "covariance1 = covariance_matrix1[0, 1]\n",
    "covariance2 = covariance_matrix2[0, 1]\n",
    "print(\"Covariance between input1 and input2:\", covariance)\n",
    "print(\"Covariance between input2 and input3:\", covariance1)\n",
    "print(\"Covariance between input3 and input2:\", covariance2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exist so using it\n",
      "Epoch [100/1000], Loss: 4.0892997346847615\n",
      "Epoch [200/1000], Loss: 3.934955867525348\n",
      "Epoch [300/1000], Loss: 3.8813124414170086\n",
      "Epoch [400/1000], Loss: 3.860799135241946\n",
      "Epoch [500/1000], Loss: 3.8518611038952892\n",
      "Epoch [600/1000], Loss: 3.8472903304740522\n",
      "Epoch [700/1000], Loss: 3.844586056745506\n",
      "Epoch [800/1000], Loss: 3.8428165686445217\n",
      "Epoch [900/1000], Loss: 3.841591034855588\n",
      "Epoch [1000/1000], Loss: 3.84071779021237\n",
      "Training Done\n",
      "Model exist so using it\n",
      "Epoch [100/1000], Loss: 4.370160071086236\n",
      "Epoch [200/1000], Loss: 4.033728218257729\n",
      "Epoch [300/1000], Loss: 3.918599000568286\n",
      "Epoch [400/1000], Loss: 3.876739439023911\n",
      "Epoch [500/1000], Loss: 3.8598460701320088\n",
      "Epoch [600/1000], Loss: 3.8519375254032626\n",
      "Epoch [700/1000], Loss: 3.847596075825612\n",
      "Epoch [800/1000], Loss: 3.844890170954855\n",
      "Epoch [900/1000], Loss: 3.843064773893162\n",
      "Epoch [1000/1000], Loss: 3.8417807126233585\n",
      "Training Done\n",
      "Testing starting\n",
      "testing over now accuracy analysis\n",
      "Percentage Accuracy: 67.33%\n"
     ]
    }
   ],
   "source": [
    "# create_plot(X_train,y_train)\n",
    "model = load_or_initialize_model(X_train, y_train)\n",
    "model=modeltrain(X_train,y_train)\n",
    "model = load_or_initialize_model(X_train,y_train)\n",
    "model = modeltrain(X_train, y_train, l3_lambda=0.01)  # You can adjust the regularization st\n",
    "modeltest(X_test,y_test,model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
