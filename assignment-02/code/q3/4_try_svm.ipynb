{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from cvxopt import matrix, solvers\n",
    "merged_data = pd.read_csv('../../data/merged_data_truncated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Sentiment1</th>\n",
       "      <th>Sentiment2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>173.800003</td>\n",
       "      <td>177.990005</td>\n",
       "      <td>173.179993</td>\n",
       "      <td>177.490005</td>\n",
       "      <td>177.490005</td>\n",
       "      <td>57224100</td>\n",
       "      <td>0.9480</td>\n",
       "      <td>0.096352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>176.809998</td>\n",
       "      <td>179.050003</td>\n",
       "      <td>175.800003</td>\n",
       "      <td>178.990005</td>\n",
       "      <td>178.990005</td>\n",
       "      <td>42390800</td>\n",
       "      <td>-0.2204</td>\n",
       "      <td>0.071875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>178.100006</td>\n",
       "      <td>179.720001</td>\n",
       "      <td>177.949997</td>\n",
       "      <td>178.389999</td>\n",
       "      <td>178.389999</td>\n",
       "      <td>43698000</td>\n",
       "      <td>0.9768</td>\n",
       "      <td>0.170134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>178.199997</td>\n",
       "      <td>179.850006</td>\n",
       "      <td>177.600006</td>\n",
       "      <td>179.800003</td>\n",
       "      <td>179.800003</td>\n",
       "      <td>47551100</td>\n",
       "      <td>0.9854</td>\n",
       "      <td>0.143628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>180.070007</td>\n",
       "      <td>182.339996</td>\n",
       "      <td>179.039993</td>\n",
       "      <td>180.710007</td>\n",
       "      <td>180.710007</td>\n",
       "      <td>56743100</td>\n",
       "      <td>0.9934</td>\n",
       "      <td>0.106639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Open        High         Low       Close   Adj Close    Volume  \\\n",
       "0  173.800003  177.990005  173.179993  177.490005  177.490005  57224100   \n",
       "1  176.809998  179.050003  175.800003  178.990005  178.990005  42390800   \n",
       "2  178.100006  179.720001  177.949997  178.389999  178.389999  43698000   \n",
       "3  178.199997  179.850006  177.600006  179.800003  179.800003  47551100   \n",
       "4  180.070007  182.339996  179.039993  180.710007  180.710007  56743100   \n",
       "\n",
       "   Sentiment1  Sentiment2  \n",
       "0      0.9480    0.096352  \n",
       "1     -0.2204    0.071875  \n",
       "2      0.9768    0.170134  \n",
       "3      0.9854    0.143628  \n",
       "4      0.9934    0.106639  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "# y_open=merged_data['Open']\n",
    "# y_close=merged_data['Close']\n",
    "y_data=merged_data.drop(columns=['High','Low','Adj Close','Volume','Sentiment1','Sentiment2','Close'])\n",
    "# print(y_data)\n",
    "# y_array=y_data.values\n",
    "# print(y_array)\n",
    "input_vector= merged_data.drop(columns=['Open','Close','Adj Close'])\n",
    "# input_array = input_vector.values\n",
    "size=len(input_vector)\n",
    "print(size)\n",
    "train_ratio=0.6\n",
    "train_size=int(size*train_ratio)\n",
    "    \n",
    "\n",
    "\n",
    "# #Splitting the data in training and testing data \n",
    "\n",
    "\n",
    "#Training data\n",
    "X_train=input_vector[:train_size]\n",
    "# print(X_train)\n",
    "y_train=y_data[:train_size]\n",
    "    \n",
    "#Testing data\n",
    "\n",
    "X_test=input_vector[train_size:]\n",
    "y_test=y_data[train_size:]\n",
    "\n",
    "#Conversion from pandas data frame to numpy data frame\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "# print(X_train)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(x1, x2, gamma='scale'):\n",
    "      # If gamma is set to 'scale', calculate it based on the number of features in x1\n",
    "    if gamma == 'scale':\n",
    "        gamma = 1 / (2e13)\n",
    "        # gamma = 1 / (2 * len(x1))\n",
    "    #this is the radial basis kernel evaluation based on norm\n",
    "    return np.exp(-gamma * np.linalg.norm(x1 - x2) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_kernel_matrix(X, gamma='scale'):\n",
    "    n_samples = X.shape[0]\n",
    "    kernel_matrix = np.zeros((n_samples, n_samples))\n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_samples):\n",
    "            kernel_matrix[i, j] = rbf_kernel(X[i], X[j], gamma)\n",
    "    return kernel_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_svr(X, y, C=1.0, epsilon=0.1, gamma='scale'):\n",
    "#     n_samples, n_features = X.shape\n",
    "\n",
    "#     # Calculate the kernel matrix\n",
    "#     kernel_matrix = calculate_kernel_matrix(X, gamma)\n",
    "\n",
    "#     # Quadratic programming problem (simplified)\n",
    "#     P = np.outer(y, y) * kernel_matrix\n",
    "#     q = -np.ones(n_samples)\n",
    "#     G = np.vstack([-np.eye(n_samples), np.eye(n_samples)])\n",
    "#     h = np.hstack([np.zeros(n_samples), np.ones(n_samples) * C])\n",
    "\n",
    "#     # Solve the optimization problem (you may replace this with a more advanced solver)\n",
    "#     # solution = np.linalg.solve(P, q, G, h)\n",
    "#     solution = solvers.qp(P, q, G, h)\n",
    "\n",
    "#     # Extract weights, intercept, and support vectors\n",
    "#     alpha = solution[:n_samples]\n",
    "#     sv_indices = np.where(alpha > 1e-5)[0]\n",
    "#     support_vectors_X = X[sv_indices]\n",
    "#     support_vectors_y = y[sv_indices]\n",
    "#     support_vectors_alpha = alpha[sv_indices]\n",
    "\n",
    "#     # Calculate weights and intercept\n",
    "#     weights = np.sum(support_vectors_alpha * support_vectors_y[:, None] *\n",
    "#                      np.array([rbf_kernel(x, support_vectors_X, gamma) for x in X.values]), axis=0)\n",
    "#     intercept = np.mean(support_vectors_y - np.dot(weights, support_vectors_X.values.T))\n",
    "\n",
    "#     return weights, intercept, {'X': support_vectors_X, 'y': support_vectors_y, 'alpha': support_vectors_alpha}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from cvxopt import matrix, solvers\n",
    "\n",
    "# def train_svr(X, y, C=1.0, epsilon=0.1, gamma='scale'):\n",
    "#     n_samples, n_features = X.shape\n",
    "\n",
    "#     # Calculate the kernel matrix\n",
    "#     kernel_matrix = calculate_kernel_matrix(X, gamma)\n",
    "\n",
    "#     # Quadratic programming problem (simplified)\n",
    "#     P = matrix(np.outer(y.flatten(), y.flatten()) * kernel_matrix)\n",
    "#     q = matrix(-np.ones(n_samples))\n",
    "#     G = matrix(np.vstack([-np.eye(n_samples), np.eye(n_samples)]))\n",
    "#     h = matrix(np.hstack([np.zeros(n_samples), np.ones(n_samples) * C]))\n",
    "\n",
    "#     # Solve the optimization problem using cvxopt\n",
    "#     sol = solvers.qp(P, q, G, h)\n",
    "\n",
    "#     # Extract Lagrange multipliers\n",
    "#     alpha = np.array(sol['x']).flatten()\n",
    "\n",
    "#     # Extract support vectors\n",
    "#     sv_indices = np.where(alpha > 1e-5)[0]\n",
    "#     support_vectors_X = X[sv_indices]\n",
    "#     support_vectors_y = y[sv_indices]\n",
    "#     support_vectors_alpha = alpha[sv_indices]\n",
    "\n",
    "# # Calculate weights and intercept\n",
    "#     weights = np.sum(support_vectors_alpha[:, None, None] *\n",
    "#                 support_vectors_y[:, :, None] *\n",
    "#                 np.array([rbf_kernel(x, support_vectors_X, gamma) for x in X]), axis=0)\n",
    "\n",
    "#     intercept = np.mean(support_vectors_y - np.sum(weights * np.array([rbf_kernel(x, support_vectors_X, gamma) for x in X]), axis=0))\n",
    "\n",
    "#     return weights, intercept, {'X': support_vectors_X, 'y': support_vectors_y, 'alpha': support_vectors_alpha} \n",
    "\n",
    "\n",
    "# from cvxopt import matrix, solvers\n",
    "\n",
    "# weights, intercept, support_vectors =train_svr(X_train, y_train, C=1.0, epsilon=0.1, gamma='scale')\n",
    "\n",
    "\n",
    "# def train_svr(X, y, C=1.0, epsilon=0.1, gamma='scale'):\n",
    "#     global P,q,G,h\n",
    "#     n_samples, n_features = X.shape\n",
    "\n",
    "#     # Normalize input features if needed\n",
    "\n",
    "#     # Calculate the kernel matrix\n",
    "#     kernel_matrix = calculate_kernel_matrix(X, gamma)\n",
    "\n",
    "#     # Quadratic programming problem (simplified)\n",
    "#     P = matrix(np.outer(y.flatten(), y.flatten()) * kernel_matrix)\n",
    "#     q = matrix(-np.ones(n_samples))\n",
    "#     G = matrix(np.vstack([-np.eye(n_samples), np.eye(n_samples)]))\n",
    "#     h = matrix(np.hstack([np.zeros(n_samples), np.ones(n_samples) * C]))\n",
    "\n",
    "#     # Solve the optimization problem using cvxopt\n",
    "#     sol = solvers.qp(P, q, G, h)\n",
    "\n",
    "#     # Extract Lagrange multipliers\n",
    "#     alpha = np.array(sol['x']).flatten()\n",
    "\n",
    "#     # Extract support vectors\n",
    "#     sv_indices = np.where(alpha > 1e-5)[0]\n",
    "#     support_vectors_X = X[sv_indices]\n",
    "#     support_vectors_y = y[sv_indices]\n",
    "#     support_vectors_alpha = alpha[sv_indices]\n",
    "\n",
    "#     # Calculate weights and intercept\n",
    "#     weights = np.sum(support_vectors_alpha[:, None, None] *\n",
    "#                      support_vectors_y[:, :, None] *\n",
    "#                      np.array([rbf_kernel(x, support_vectors_X, gamma) for x in X]), axis=0)\n",
    "\n",
    "#     intercept = np.mean(support_vectors_y - np.sum(weights * np.array([rbf_kernel(x, support_vectors_X, gamma) for x in X]), axis=0))\n",
    "\n",
    "#     return weights, intercept, {'X': support_vectors_X, 'y': support_vectors_y, 'alpha': support_vectors_alpha}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_svr(X_test, weights, intercept, support_vectors, gamma='scale'):\n",
    "    \n",
    "    kernel_values = np.array([rbf_kernel(x, support_vectors['X'], gamma) for x in X_test])\n",
    "    print(type(weights))\n",
    "    print(type(kernel_values))\n",
    "    kernel_values = kernel_values.reshape(1, -1)\n",
    "    weights = weights.T\n",
    "\n",
    "    # Perform the dot product and add the intercept\n",
    "    predictions = np.dot(weights, kernel_values) + intercept\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights, intercept, support_vectors =train_svr(X_train, y_train, C=1.0, epsilon=0.1, gamma='scale')\n",
    "# y_pred=predict_svr(X_test,weights,intercept,support_vectors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(y_test,y_pred):\n",
    "    # y_true = y_test  # Replace true_values with the actual values from your dataset\n",
    "    # y_pred = predict_svr(X_test, weights, intercept, support_vectors, gamma)\n",
    "    # print(y_pred)\n",
    "    # print(y_test)\n",
    "# Calculate the mean squared error (MSE)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f'Mean Squared Error (MSE): {mse}')\n",
    "\n",
    "# Calculate the mean absolute error (MAE)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    print(f'Mean Absolute Error (MAE): {mae}')\n",
    "\n",
    "# Calculate the R-squared (R2) score\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f'R-squared (R2) Score: {r2}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.1222e-04 -1.0087e+01  4e+01  1e+00  2e-14\n",
      " 1: -2.7769e-05 -2.6650e+00  3e+00  2e-02  1e-14\n",
      " 2:  1.4228e-04 -4.0403e-02  4e-02  3e-04  1e-14\n",
      " 3:  6.4892e-05 -7.0610e-04  8e-04  3e-06  5e-16\n",
      " 4: -3.9899e-05 -9.7333e-05  6e-05  2e-08  4e-16\n",
      " 5: -4.8067e-05 -5.1697e-05  4e-06  2e-16  2e-16\n",
      " 6: -4.8964e-05 -4.9596e-05  6e-07  2e-16  1e-16\n",
      " 7: -4.9274e-05 -4.9340e-05  7e-08  1e-16  6e-17\n",
      "Optimal solution found.\n"
     ]
    }
   ],
   "source": [
    "X=X_train\n",
    "y=y_train\n",
    "C=1.0\n",
    "epsilon=0.1\n",
    "gamma='scale'\n",
    "\n",
    "# weights, intercept, support_vectors =train_svr(X_train, y_train, C=1.0, epsilon=0.1, gamma='scale')\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "if 2:\n",
    "    # Normalize input features if needed\n",
    "\n",
    "    # Calculate the kernel matrix\n",
    "    kernel_matrix = calculate_kernel_matrix(X, gamma)\n",
    "\n",
    "    # Quadratic programming problem (simplified)\n",
    "    P = matrix(np.outer(y.flatten(), y.flatten()) * kernel_matrix)\n",
    "    q = matrix(-np.ones(n_samples))\n",
    "    G = matrix(np.vstack([-np.eye(n_samples), np.eye(n_samples)]))\n",
    "    h = matrix(np.hstack([np.zeros(n_samples), np.ones(n_samples) * C]))\n",
    "\n",
    "    # Solve the optimization problem using cvxopt\n",
    "    sol = solvers.qp(P, q, G, h)\n",
    "\n",
    "    # Extract Lagrange multipliers\n",
    "    alpha = np.array(sol['x']).flatten()\n",
    "\n",
    "    # Extract support vectors\n",
    "    sv_indices = np.where(alpha > 1e-5)[0]\n",
    "    support_vectors_X = X[sv_indices]\n",
    "    support_vectors_y = y[sv_indices]\n",
    "    support_vectors_alpha = alpha[sv_indices]\n",
    "\n",
    "    # Calculate weights and intercept\n",
    "    weights = np.sum(support_vectors_alpha[:, None, None] *\n",
    "                     support_vectors_y[:, :, None] *\n",
    "                     np.array([rbf_kernel(x, support_vectors_X, gamma) for x in X]), axis=0)\n",
    "\n",
    "    intercept = np.mean(support_vectors_y - np.sum(weights * np.array([rbf_kernel(x, support_vectors_X, gamma) for x in X]), axis=0))\n",
    "\n",
    "    # return weights, intercept, {'X': support_vectors_X, 'y': support_vectors_y, 'alpha': support_vectors_alpha}\n",
    "\n",
    "support_vectors={'X': support_vectors_X, 'y': support_vectors_y, 'alpha': support_vectors_alpha}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1. -0. -0. -0. -0. -0. -0. -0. -0. -0.]\n",
      " [-0. -1. -0. -0. -0. -0. -0. -0. -0. -0.]\n",
      " [-0. -0. -1. -0. -0. -0. -0. -0. -0. -0.]\n",
      " [-0. -0. -0. -1. -0. -0. -0. -0. -0. -0.]\n",
      " [-0. -0. -0. -0. -1. -0. -0. -0. -0. -0.]\n",
      " [-0. -0. -0. -0. -0. -1. -0. -0. -0. -0.]\n",
      " [-0. -0. -0. -0. -0. -0. -1. -0. -0. -0.]\n",
      " [-0. -0. -0. -0. -0. -0. -0. -1. -0. -0.]\n",
      " [-0. -0. -0. -0. -0. -0. -0. -0. -1. -0.]\n",
      " [-0. -0. -0. -0. -0. -0. -0. -0. -0. -1.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.],\n",
       "       [-1.],\n",
       "       [-1.],\n",
       "       [-1.],\n",
       "       [-1.],\n",
       "       [-1.],\n",
       "       [-1.],\n",
       "       [-1.],\n",
       "       [-1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(G)\n",
    "# g=np.array(G)\n",
    "print(g)\n",
    "g.shape\n",
    "# print(h)\n",
    "# G*X[0]\n",
    "# C\n",
    "# X.shape\n",
    "x1=np.ones((10,1))\n",
    "# x1\n",
    "np.dot(g,x1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.02e+04  5.13e-01  3.30e+00  2.88e+02  3.09e+04  5.87e+03  1.01e+04 ... ]\n",
      "[ 5.13e-01  3.13e+04  2.89e+04  8.32e+03  1.07e+00  5.41e+02  1.85e+02 ... ]\n",
      "[ 3.30e+00  2.89e+04  3.17e+04  1.51e+04  6.47e+00  1.63e+03  6.44e+02 ... ]\n",
      "[ 2.88e+02  8.32e+03  1.51e+04  3.18e+04  4.69e+02  1.53e+04  9.18e+03 ... ]\n",
      "[ 3.09e+04  1.07e+00  6.47e+00  4.69e+02  3.24e+04  7.95e+03  1.30e+04 ... ]\n",
      "[ 5.87e+03  5.41e+02  1.63e+03  1.53e+04  7.95e+03  3.29e+04  3.02e+04 ... ]\n",
      "[ 1.01e+04  1.85e+02  6.44e+02  9.18e+03  1.30e+04  3.02e+04  3.12e+04 ... ]\n",
      "[ 3.05e+04  3.20e-01  2.15e+00  2.12e+02  3.08e+04  4.92e+03  8.80e+03 ... ]\n",
      "[ 2.26e+04  1.47e+01  6.85e+01  2.32e+03  2.60e+04  1.83e+04  2.41e+04 ... ]\n",
      "[ 2.47e+04  1.92e-02  1.62e-01  3.14e+01  2.28e+04  1.44e+03  3.11e+03 ... ]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.00000000e+00, 1.66793448e-05, 1.06457037e-04, 9.29436989e-03,\n",
       "       9.88498603e-01, 1.86325441e-01, 3.30271736e-01, 9.94722968e-01,\n",
       "       7.38963903e-01, 8.05678522e-01])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(P)\n",
    "\n",
    "print(P)\n",
    "# print(matrix((np.outer(y.flatten(), y.flatten()) * kernel_matrix)))\n",
    "# print(kernel_matrix)\n",
    "X[0]\n",
    "X[1]\n",
    "rbf_kernel(X[0], X[1], gamma)\n",
    "np.linalg.norm(X[0] - X[1]) ** 2\n",
    "\n",
    "1 / (2 * len(X[0]))*np.linalg.norm(X[0] - X[1]) ** 2\n",
    "# len(X[0])\n",
    "\n",
    "# print(X[1])\n",
    "# X[0]\n",
    "rbf_kernel(X[0], X[1], gamma)\n",
    "kernel_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred=predict_svr(X_test,weights,intercept,support_vectors)\n",
    "# y_pred=y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([173.05000305])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 7)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_test.shape\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [7, 10]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\pande\\OneDrive\\Desktop\\code\\ML\\ELL409-Machine-Learning-Coursework\\assignment-02\\code\\q3\\4_try_svm.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/pande/OneDrive/Desktop/code/ML/ELL409-Machine-Learning-Coursework/assignment-02/code/q3/4_try_svm.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m analysis(y_test,y_pred)\n",
      "\u001b[1;32mc:\\Users\\pande\\OneDrive\\Desktop\\code\\ML\\ELL409-Machine-Learning-Coursework\\assignment-02\\code\\q3\\4_try_svm.ipynb Cell 19\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pande/OneDrive/Desktop/code/ML/ELL409-Machine-Learning-Coursework/assignment-02/code/q3/4_try_svm.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39manalysis\u001b[39m(y_test,y_pred):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pande/OneDrive/Desktop/code/ML/ELL409-Machine-Learning-Coursework/assignment-02/code/q3/4_try_svm.ipynb#X42sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m# y_true = y_test  # Replace true_values with the actual values from your dataset\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pande/OneDrive/Desktop/code/ML/ELL409-Machine-Learning-Coursework/assignment-02/code/q3/4_try_svm.ipynb#X42sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m# y_pred = predict_svr(X_test, weights, intercept, support_vectors, gamma)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pande/OneDrive/Desktop/code/ML/ELL409-Machine-Learning-Coursework/assignment-02/code/q3/4_try_svm.ipynb#X42sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m# print(y_pred)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pande/OneDrive/Desktop/code/ML/ELL409-Machine-Learning-Coursework/assignment-02/code/q3/4_try_svm.ipynb#X42sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# print(y_test)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pande/OneDrive/Desktop/code/ML/ELL409-Machine-Learning-Coursework/assignment-02/code/q3/4_try_svm.ipynb#X42sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Calculate the mean squared error (MSE)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/pande/OneDrive/Desktop/code/ML/ELL409-Machine-Learning-Coursework/assignment-02/code/q3/4_try_svm.ipynb#X42sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     mse \u001b[39m=\u001b[39m mean_squared_error(y_test, y_pred)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pande/OneDrive/Desktop/code/ML/ELL409-Machine-Learning-Coursework/assignment-02/code/q3/4_try_svm.ipynb#X42sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mMean Squared Error (MSE): \u001b[39m\u001b[39m{\u001b[39;00mmse\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pande/OneDrive/Desktop/code/ML/ELL409-Machine-Learning-Coursework/assignment-02/code/q3/4_try_svm.ipynb#X42sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Calculate the mean absolute error (MAE)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pande\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\pande\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:474\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[39m@validate_params\u001b[39m(\n\u001b[0;32m    405\u001b[0m     {\n\u001b[0;32m    406\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m\"\u001b[39m\u001b[39marray-like\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    415\u001b[0m     y_true, y_pred, \u001b[39m*\u001b[39m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, multioutput\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39muniform_average\u001b[39m\u001b[39m\"\u001b[39m, squared\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    416\u001b[0m ):\n\u001b[0;32m    417\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Mean squared error regression loss.\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \n\u001b[0;32m    419\u001b[0m \u001b[39m    Read more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[39m    0.825...\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 474\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[39m=\u001b[39m _check_reg_targets(\n\u001b[0;32m    475\u001b[0m         y_true, y_pred, multioutput\n\u001b[0;32m    476\u001b[0m     )\n\u001b[0;32m    477\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    478\u001b[0m     output_errors \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39maverage((y_true \u001b[39m-\u001b[39m y_pred) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, weights\u001b[39m=\u001b[39msample_weight)\n",
      "File \u001b[1;32mc:\\Users\\pande\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:99\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_reg_targets\u001b[39m(y_true, y_pred, multioutput, dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     66\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \n\u001b[0;32m     68\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[39m        correct keyword.\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m     check_consistent_length(y_true, y_pred)\n\u001b[0;32m    100\u001b[0m     y_true \u001b[39m=\u001b[39m check_array(y_true, ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    101\u001b[0m     y_pred \u001b[39m=\u001b[39m check_array(y_pred, ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\pande\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:409\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    407\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    408\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    410\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    412\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [7, 10]"
     ]
    }
   ],
   "source": [
    "analysis(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([176.94999695, 176.94999695, 176.94999695, 176.94999695,\n",
       "       176.94999695, 176.94999695, 176.94999695])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([176.94999695, 176.94999695, 176.94999695, 176.94999695,\n",
       "       176.94999695, 176.94999695, 176.94999695])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([176.94999695, 176.94999695, 176.94999695, 176.94999695,\n",
       "       176.94999695, 176.94999695, 176.94999695])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape\n",
    "y_pred.shape\n",
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Assuming merged_data has columns like 'Date', 'Adj Close', 'Open', 'News_Sentiment', and other relevant features\n",
    "# features = merged_data[['High', 'Low', 'Volume','Sentiment1','Sentiment2']]  # Adjust features accordingly\n",
    "# # target = merged_data[['Open','Close']]  # Assuming you have a column 'Next_Day_Adj_Close'\n",
    "# target = merged_data['Close']\n",
    "\n",
    "# # Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# def rbf_kernel(X1, X2, gamma='auto'):\n",
    "#     if gamma == 'auto':\n",
    "#         # gamma = 1 / len(X1[0])\n",
    "#         gamma = 1 / X1.shape[0]\n",
    "#     return np.exp(-gamma * np.linalg.norm(X1 - X2) ** 2)\n",
    "\n",
    "# def fit_svr(X, y, kernel='rbf', C=1.0, epsilon=0.1, gamma='auto'):\n",
    "#     n_samples, n_features = X.shape\n",
    "\n",
    "#     # Compute the kernel matrix\n",
    "#     kernel_matrix = np.zeros((n_samples, n_samples))\n",
    "#     for i in range(n_samples):\n",
    "#         for j in range(n_samples):\n",
    "#             if kernel == 'rbf':\n",
    "#                 kernel_matrix[i, j] = rbf_kernel(X[i], X[j], gamma)\n",
    "\n",
    "#     # Quadratic programming problem to solve the dual problem\n",
    "#     P = np.outer(y, y) * kernel_matrix\n",
    "#     q = -np.ones(n_samples)\n",
    "#     G = np.vstack((-np.eye(n_samples), np.eye(n_samples)))\n",
    "#     h = np.hstack((np.zeros(n_samples), np.ones(n_samples) * C))\n",
    "\n",
    "#     # Solve the quadratic programming problem\n",
    "#     alpha = np.linalg.solve(P, q)\n",
    "\n",
    "#     # Extract support vectors\n",
    "#     sv_indices = alpha > 1e-5\n",
    "#     support_vectors = X[sv_indices]\n",
    "#     support_vector_labels = y[sv_indices]\n",
    "\n",
    "#     # Calculate weights and bias\n",
    "#     weights = np.sum(alpha[sv_indices][:, np.newaxis] * support_vector_labels[:, np.newaxis] * support_vectors, axis=0)\n",
    "#     bias = np.mean(support_vector_labels - np.dot(support_vectors, weights))\n",
    "\n",
    "#     return support_vectors, support_vector_labels, weights, bias\n",
    "\n",
    "# # def predict_svr(X, support_vectors, weights, bias, kernel='rbf', gamma='auto'):\n",
    "# #     if kernel == 'rbf':\n",
    "# #         kernel_values = np.array([rbf_kernel(X, sv, gamma) for sv in support_vectors])\n",
    "# #     else:\n",
    "# #         kernel_values = np.dot(X, support_vectors.T)\n",
    "\n",
    "# #     return np.dot(kernel_values, weights) + bias\n",
    "# def predict_svr(X, support_vectors, weights, bias, kernel='rbf', gamma='auto'):\n",
    "#     if kernel == 'rbf':\n",
    "#         kernel_values = np.array([rbf_kernel(X_i, sv, gamma) for X_i in X for sv in support_vectors])\n",
    "#     else:\n",
    "#         kernel_values = np.dot(X, support_vectors.T)\n",
    "\n",
    "#     return np.dot(kernel_values, weights) + bias\n",
    "\n",
    "# # Usage:\n",
    "# support_vectors, sv_labels, sv_weights, sv_bias = fit_svr(X_train.values, y_train.values)\n",
    "# y_pred = predict_svr(X_test.values, support_vectors, sv_weights, sv_bias)\n",
    "\n",
    "# def analysis(y_test,y_pred):\n",
    "#     # y_true = y_test  # Replace true_values with the actual values from your dataset\n",
    "#     # y_pred = predict_svr(X_test, weights, intercept, support_vectors, gamma)\n",
    "\n",
    "# # Calculate the mean squared error (MSE)\n",
    "#     mse = mean_squared_error(y_test, y_pred)\n",
    "#     print(f'Mean Squared Error (MSE): {mse}')\n",
    "\n",
    "# # Calculate the mean absolute error (MAE)\n",
    "#     mae = mean_absolute_error(y_test, y_pred)\n",
    "#     print(f'Mean Absolute Error (MAE): {mae}')\n",
    "\n",
    "# # Calculate the R-squared (R2) score\n",
    "#     r2 = r2_score(y_test, y_pred)\n",
    "#     print(f'R-squared (R2) Score: {r2}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
